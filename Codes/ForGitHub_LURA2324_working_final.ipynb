{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LURA Project: Enhancing the Machine Learning-based Detection of Long-lasting Continuous Gravitational Waves\n",
        "\n",
        "This is the Jupyter Notebook of the LURA project implemented here in Google Colab.\n",
        "\n",
        "- NASA Primary Grant Number: 80NSSC20M0110\n",
        "- LSU/LaSPACE Sub-award Number: GR00013285\n",
        "- Project Period: 15-Aug-23 to 14-Aug-24\n",
        "- Mentored LURA Student: Lorin Bernard\n",
        "- Faculty PI: Dr. Dhan Lord B. Fortela\n",
        "- LaSPACE: https://laspace.lsu.edu//\n",
        "- Louisiana NASA EPSCoR: https://lanasaepscor.lsu.edu/"
      ],
      "metadata": {
        "id": "cqDcw0K5em_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation of Continupus GW data\n",
        "- Using LALSuite and PyFstate. Installing PyFstate will also install LALSuite\n",
        "- Run the codes below. Just use the basic Python runetime of Google Colab because GPU cannot accelarae PyFstate and LALSuite\n"
      ],
      "metadata": {
        "id": "MUmTjqVDc6mC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuGRu6Qy-9Kg"
      },
      "outputs": [],
      "source": [
        "# need to install PyFstat becuase Colab does not have it in its repo\n",
        "\n",
        "!pip install pyfstat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0E0xDod_XyL"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "os.environ[\"DISPLAY\"] = \"1.0\"\n",
        "\n",
        "import h5py\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "import pyfstat\n",
        "logger = pyfstat.set_up_logger(outdir=\"pyfstat_log\", label=\"1_generating_signals\", log_level=\"WARNING\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read arguments:\n",
        "    msg = \"Generate dataset containing continuous gravitational wave data.\\n\"\n",
        "    msg += \"Example usage:\\n\"\n",
        "    msg += \"> python generate_data.py --sensitivity 10.0 --num_signals 1000\"\n",
        "\n",
        "    #sensitivity\n",
        "    sensitivity = 5\n",
        "    #num_signals = args.num_signals\n",
        "    num_signals = 50\n",
        "\n",
        "\n",
        "    out_dir = \"./data/\"\n",
        "    dataset_name = f\"data_sensitivity_{int(sensitivity)}\"\n",
        "\n",
        "    print(\"=============================================\")\n",
        "    print(f\"Generating dataset {dataset_name} with sensitivity {sensitivity} and {num_signals} files.\\n\")\n",
        "\n",
        "    if not os.path.isdir(out_dir):\n",
        "        os.mkdir(out_dir)\n",
        "\n",
        "    if not os.path.isdir(os.path.join(out_dir, dataset_name)):\n",
        "        os.mkdir(os.path.join(out_dir, dataset_name))\n",
        "\n",
        "\n",
        "    # Parameters for Pyfstat:\n",
        "\n",
        "    # These parameters describe background noise and data format\n",
        "    writer_kwargs = {\n",
        "        \"label\": \"single_detector_gaussian_noise\",\n",
        "        \"outdir\": \"Generated_data\",\n",
        "        \"tstart\": 1238166018,  # Starting time of the observation [GPS time]\n",
        "        \"duration\": 120 * 86400,  # Duration [seconds]\n",
        "        \"detectors\": \"H1,L1\",  # Detector to simulate, in this case LIGO Hanford\n",
        "        \"Band\": 0.18,  # Frequency band-width around F0 [Hz]\n",
        "        \"sqrtSX\": 1e-15,  # Single-sided Amplitude Spectral Density of the noise\n",
        "        \"Tsft\": 1800,  # Fourier transform time duration\n",
        "        \"SFTWindowType\": \"tukey\",  # Window function to compute short Fourier transforms\n",
        "        \"SFTWindowParam\": 0.01,  # Parameter associated to the window function\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    # Sample signal parameters from a specific population:\n",
        "    # Vary these parameters to generate varied data for CNN training\n",
        "\n",
        "    signal_parameters_generator = pyfstat.AllSkyInjectionParametersGenerator(\n",
        "        priors={\n",
        "            \"tref\": writer_kwargs[\"tstart\"],\n",
        "            \"F0\": {\"stats.uniform\": {\"loc\": 390, \"scale\": 50}}, # Central frequency of the band to be generated [Hz]\n",
        "            \"F1\": {\"stats.uniform\": {\"loc\": -1e-10, \"scale\": 2e-10}},\n",
        "            \"F2\": 0,\n",
        "            \"Alpha\": 2.3,\n",
        "            \"Delta\": 1.8,\n",
        "            \"psi\": 0,\n",
        "            \"phi\": 0,\n",
        "            \"cosi\": 0.3,\n",
        "            \"h0\": writer_kwargs[\"sqrtSX\"]/sensitivity,\n",
        "            **pyfstat.injection_parameters.isotropic_amplitude_distribution,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    columns = ['id', 'target', 'SNR', 'sqrtSX', 'F0', 'F1', 'h0', 'alpha', 'delta', 'psi', 'phi', 'cosi']\n",
        "    df = pd.DataFrame(columns=columns, index=None)\n",
        "\n",
        "    print(columns[:7])\n",
        "\n",
        "    i_signal = 0\n",
        "\n",
        "    while i_signal < num_signals:\n",
        "\n",
        "        file_id = f\"{i_signal:05}\"\n",
        "\n",
        "        # Draw signal parameters.\n",
        "        params = signal_parameters_generator.draw()\n",
        "        writer_kwargs[\"outdir\"] = f\"Generated_SFT_data_tmp/Signal{i_signal}\"\n",
        "        writer_kwargs[\"label\"] = f\"Signal{i_signal}\"\n",
        "\n",
        "        target = stats.bernoulli(0.5).rvs()  # 0 or 1\n",
        "        params[\"h0\"] *= target               # set h0 randomly to zero to generate only noise\n",
        "\n",
        "        writer = pyfstat.Writer(**writer_kwargs, **params)\n",
        "        try:\n",
        "            writer.make_data()\n",
        "        except:\n",
        "            print(f\"couldn't write {file_id}. Trying again with new set of parameters\")\n",
        "            continue  # do not increment iterator\n",
        "\n",
        "        # SNR can be compute from a set of SFTs for a specific set\n",
        "        # of parameters as follows:\n",
        "        snr_ = pyfstat.SignalToNoiseRatio.from_sfts(F0=writer.F0, sftfilepath=writer.sftfilepath)\n",
        "\n",
        "        squared_snr = snr_.compute_snr2(\n",
        "            Alpha=writer.Alpha,\n",
        "            Delta=writer.Delta,\n",
        "            psi=writer.psi,\n",
        "            phi=writer.phi,\n",
        "            h0=writer.h0,\n",
        "            cosi=writer.cosi\n",
        "        )\n",
        "\n",
        "        SNR = np.sqrt(squared_snr)\n",
        "\n",
        "        # Data can be read as a numpy array using PyFstat\n",
        "        frequency, timestamps, amplitudes = pyfstat.utils.get_sft_as_arrays(writer.sftfilepath)\n",
        "\n",
        "    #     columns = ['file_id', 'target', 'SNR', 'sqrtSX', 'F0', 'F1', 'h0', 'alpha', 'delta', 'psi', 'phi', 'cosi']\n",
        "        vals = [file_id, target, SNR, writer.sqrtSX, writer.F0, writer.F1, writer.h0, writer.Alpha, writer.Delta, writer.psi, writer.phi, writer.cosi]\n",
        "        df.loc[i_signal] = vals\n",
        "        print(vals[:7])\n",
        "\n",
        "        # save hdf5 files:\n",
        "        out_file = os.path.join(out_dir, dataset_name, file_id+\".hdf5\")\n",
        "        with h5py.File(out_file, \"w\") as f:\n",
        "\n",
        "            H1 = f.create_group(file_id+'/H1')\n",
        "            L1 = f.create_group(file_id+'/L1')\n",
        "\n",
        "            f.create_dataset(file_id+'/frequency_Hz', data=frequency)\n",
        "\n",
        "            H1.create_dataset('SFTs', data=amplitudes[\"H1\"])\n",
        "            H1.create_dataset('timestamps_GPS', data=timestamps[\"H1\"])\n",
        "\n",
        "            L1.create_dataset('SFTs', data=amplitudes[\"L1\"])\n",
        "            L1.create_dataset('timestamps_GPS', data=timestamps[\"L1\"])\n",
        "\n",
        "        i_signal += 1\n",
        "\n",
        "\n",
        "    # save labels as csv file:\n",
        "    df.to_csv(out_dir+dataset_name+\"_labels.csv\", index=False)\n",
        "\n",
        "    print()\n",
        "    print(f\"Data saved to {os.path.join(out_dir, dataset_name)}\")\n",
        "\n",
        "    print()\n",
        "    print(\"Deleting temporary SFT data in Generated_SFT_data_tmp/\")\n",
        "    os.system(\"rm -r Generated_SFT_data_tmp/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIOARpwxWx_p"
      },
      "outputs": [],
      "source": [
        "# To downlowd dataset, first zip the \"data\" folder\n",
        "!zip -r /content/out_file.zip /content/data\n",
        "\n",
        "# Then download the zipped \"data\" folder now as \"out_file.zip\"\n",
        "from google.colab import files\n",
        "files.download(\"/content/out_file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Training\n",
        "\n",
        "- Assumption: You want to use Pytorch from Google Colab so it is best to upload your Continuous GW data to Google Drive and access your Drive folder from Colab or mount your Drive folder to Colab\n",
        "- Make sure to upload all download zipped file to Google Drive\n",
        "- Mount the Drive folder using the code below. An series of authorization and authentication dialogue boxes will prompt you to answer. Complete these.\n",
        "- Then make sure to copy the link to the specific folder and paste them the the pertinent code lines below requiring access to your data in Drive."
      ],
      "metadata": {
        "id": "ibB3l2dWb2ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "8OXVJ8TQyGHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "ZzxUvg6aJ4si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlhKBnudEYBi"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import copy\n",
        "import h5py\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import utils # make sure to uplaod the 'utils' folder from the GitHub repo of our LURA project\n",
        "import utils.ML, utils.models\n",
        "from utils.data import G2NetDataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read arguments:\n",
        "    msg = \"Train a model using dataset present in data/\\n\"\n",
        "    msg += \"Example usage:\\n\"\n",
        "    msg += \"> python train_model.py --model_name CNN --num_epochs 15 --batch_size 32 --device mps\"\n",
        "    #parser = argparse.ArgumentParser(description=msg, formatter_class=argparse.RawTextHelpFormatter)\n",
        "\n",
        "    #parser.add_argument('--model_name', required=True)\n",
        "    #parser.add_argument('--num_epochs', default='10', type=int)\n",
        "    #parser.add_argument('--batch_size', default='32', type=int)\n",
        "    #parser.add_argument('--device', default='mps')\n",
        "    #parser.add_argument('--num_workers', default='1', type=int)\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    #model_name = args.model_name\n",
        "    model_name = \"CNN\"\n",
        "    #num_epochs = args.num_epochs\n",
        "    num_epochs = 15\n",
        "    #batch_size = args.batch_size\n",
        "    batch_size = 32\n",
        "    #device = args.device\n",
        "    device =  \"cpu\" # 'cuda'\n",
        "    #num_workers = args.num_workers\n",
        "    num_workers = 1\n",
        "\n",
        "    # Load data:\n",
        "    data_path = '/content/drive/MyDrive/Colab Notebooks/LURA_2324/data_all/' # use your own Drive path\n",
        "\n",
        "    # List all datasets in data_all/:\n",
        "    #dataset_list = [d for d in os.listdir(data_path) if os.path.isdir('data_all/'+d)]\n",
        "\n",
        "    # or manually specify which data set in your Drive folder you want to use:\n",
        "    dataset_list = [\"data_sensitivity_5\",\"data_sensitivity_6\",\n",
        "                    \"data_sensitivity_61\",\n",
        "                    \"data_sensitivity_7\",\"data_sensitivity_8\",\n",
        "                    \"data_sensitivity_9\",\"data_sensitivity_95\",\n",
        "                    \"data_sensitivity_10\",\"data_sensitivity_12\",\n",
        "                    \"data_sensitivity_13\",\"data_sensitivity_14\",\n",
        "                    \"data_sensitivity_15\"]\n",
        "\n",
        "    print(f\"datasets found for train/validation/test:\\n{dataset_list}\")\n",
        "\n",
        "    normalize = True  # normalize all spectrograms to zero-mean unit variance\n",
        "    augment = True    # data augmentation\n",
        "\n",
        "    data = torch.utils.data.ConcatDataset(\n",
        "        [G2NetDataset(data_path, dataset_name, normalize=normalize, augment=augment) for dataset_name in dataset_list]\n",
        "        )\n",
        "\n",
        "    train_val_test_split = [0.6, 0.2, 0.2] # you can vary this default we used in our LURA work\n",
        "\n",
        "    # split training data into train and validation sets:\n",
        "    train_split, val_split, test_split = torch.utils.data.random_split(data, train_val_test_split, generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    num_data = len(data)\n",
        "    num_train, num_val, num_test = len(train_split), len(val_split), len(test_split)\n",
        "\n",
        "    print(f\"\\nDataset size: {num_data}\")\n",
        "    print(f\"train/val/test split: {train_val_test_split[0]}/{train_val_test_split[1]}/{train_val_test_split[2]}\")\n",
        "    print(f\"Number of train/val/test samples: {num_train}/{num_val}/{num_test}\")\n",
        "\n",
        "    # Create data loaders.\n",
        "    train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_dataloader = DataLoader(test_split, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load model:\n",
        "    if model_name == \"CNN\":\n",
        "        model = utils.models.CNN().to(device)\n",
        "    elif model_name == \"CNN_2\":\n",
        "        model = utils.models.CNN_v2().to(device)\n",
        "    elif model_name == \"EfficientNet\":\n",
        "        model = utils.models.EfficientNet(freeze_blocks=False).to(device)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{model_name} not implemented. Choose among 'CNN', 'CNN_2', or 'EfficientNet'.\")\n",
        "    print(f\"\\nTraining model {model_name}:\")\n",
        "\n",
        "    # Freeze moving average parameters:\n",
        "    model.moving_average.weight.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss_log, val_loss_log = utils.ML.train_model(model,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        num_epochs,\n",
        "        save_model=True,\n",
        "        model_name=model_name,\n",
        "        device=device,\n",
        "        verbose=True)\n",
        "\n",
        "    # Evaluate model:\n",
        "\n",
        "    # Load the model with best validation AUC:\n",
        "    model = torch.jit.load('saved_models/'+model_name+'.pt', map_location='cpu') # make sure to creat the \"saved_models\" folder in \"content\" folder of your colab notebook runtime\n",
        "    model.to(device)\n",
        "\n",
        "    test_loss, test_acc, test_auc = utils.ML.evaluate_model(model, test_dataloader, loss_fn, device=device)\n",
        "    print(f\"Test loss: {test_loss:.3f}, test accuracy: {test_acc:.3f}, test AUC: {test_auc:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MUmTjqVDc6mC",
        "ibB3l2dWb2ip"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}